```torch.nn.DataParallel``` 是 PyTorch 中的一个模块，用于在单台机器的多个 GPU 上进行并行计算。它可以让你轻松地将模型的计算分散到多个 GPU 上，从而加速训练过程。

工作原理：

1. 模型复制：

+ 在每个 GPU 上复制模型。

+ 每个 GPU 会接收到输入数据的不同部分（批次数据会在多个 GPU 上分配）。

2. 前向传播：

+ 每个 GPU 上独立执行前向传播，计算其数据部分的输出。

+ 所有 GPU 上的输出会被收集并合并成一个张量（通常是通过 torch.cat 连接）。

3. 反向传播：

+ 在反向传播过程中，每个 GPU 计算并保存它自己的梯度。

+ 所有 GPU 上的梯度会被同步并进行平均，确保每个 GPU 上的模型参数得到一致的更新。

4. 性能：

+ 训练过程被有效地并行化，可以在较大的数据集上加速训练。

+ 当模型或数据集足够大时，跨多个 GPU 进行计算会显著提高性能。
