## 1️⃣ ```Tensor.stride()``` 是什么？

```stride()``` 返回一个 **元组 (tuple)**，表示 **沿每个维度的步幅（stride）**，即从内存中沿每个维度移动一个元素需要跳过多少个存储位置。

换句话说，它告诉你 **在底层存储中，索引某个维度的元素时要跳过多少个元素**。

## 2️⃣ 举例说明

```python3
import torch

# 创建一个 2x3 的张量
x = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])

print("x.shape:", x.shape)        # torch.Size([2, 3])
print("x.stride():", x.stride())  # 输出 (3, 1)
```

解释：

+ ```x.shape``` 是 (2, 3) → 2 行 3 列

+ ```x.stride()``` 是 (3, 1) → 内存布局中：

  + 沿第 0 维（行）移动 1 个索引，要跳过 3 个元素（因为每行有 3 个元素）

  + 沿第 1 维（列）移动 1 个索引，只跳过 1 个元素

## 3️⃣ stride 的意义

+ PyTorch 中的张量通常是 **连续的 (contiguous)**，内存按行优先存储（C-style）
+ ```stride``` 决定了 ```view()```, ```reshape()``` 等操作如何在不复制数据的情况下重排张量
+ 如果 ```stride```中的值不满足连续性规则，张量可能是 非连续 (non-contiguous)，这时需要调用 ```x.contiguous()``` 来获得连续张量

## 4️⃣ 直观理解

假设内存里 2x3 张量是这样排列的：

```
内存顺序: [1, 2, 3, 4, 5, 6]
```

+ stride(0) = 3 → 跳过 3 个元素才能到下一行

+ stride(1) = 1 → 跳过 1 个元素才能到下一列
