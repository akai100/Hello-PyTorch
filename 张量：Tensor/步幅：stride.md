## 1️⃣ ```Tensor.stride()``` 是什么？

```stride()``` 返回一个 **元组 (tuple)**，表示 **沿每个维度的步幅（stride）**，即从内存中沿每个维度移动一个元素需要跳过多少个存储位置。

换句话说，它告诉你 **在底层存储中，索引某个维度的元素时要跳过多少个元素**。

## 2️⃣ 举例说明

```python3
import torch

# 创建一个 2x3 的张量
x = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])

print("x.shape:", x.shape)        # torch.Size([2, 3])
print("x.stride():", x.stride())  # 输出 (3, 1)
```

解释：

+ ```x.shape``` 是 (2, 3) → 2 行 3 列

+ ```x.stride()``` 是 (3, 1) → 内存布局中：

  + 沿第 0 维（行）移动 1 个索引，要跳过 3 个元素（因为每行有 3 个元素）

  + 沿第 1 维（列）移动 1 个索引，只跳过 1 个元素

## 3️⃣ stride 的意义

+ PyTorch 中的张量通常是 **连续的 (contiguous)**，内存按行优先存储（C-style）
+ ```stride``` 决定了 ```view()```, ```reshape()``` 等操作如何在不复制数据的情况下重排张量
+ 如果 ```stride```中的值不满足连续性规则，张量可能是 非连续 (non-contiguous)，这时需要调用 ```x.contiguous()``` 来获得连续张量

### ```stride``` 决定连续性

假设有二维张量```x```:

```python3
x = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])
print(x.stride())  # (3, 1)
```

解释：

+ (3, 1) 表示：

  + 行步长 = 3 → 跨一行需要跳 3 个元素
  + 列步长 = 1 → 跨一列只跳 1 个元素

+ x 是连续的，因为内存顺序是 [1,2,3,4,5,6]，正好按行展开。

**转置后**

```python3
y = x.t()
print(y.stride())  # (1, 3)
```

+ 行步长 = 1 → 跨一行只跳 1 个元素

+ 列步长 = 3 → 跨一列跳 3 个元素

内存布局仍然是 ```[1,2,3,4,5,6]```，数据没动，但是访问顺序变了：

+ 访问 y[0,0] → x[0,0] = 1 ✅

+ 访问 y[0,1] → x[1,0] = 4 ✅

为了访问 y 的一行元素，需要跳跃底层内存，**不再是线性连续** → is_contiguous() == False。

## 4️⃣ 直观理解

假设内存里 2x3 张量是这样排列的：

```
内存顺序: [1, 2, 3, 4, 5, 6]
```

+ stride(0) = 3 → 跳过 3 个元素才能到下一行

+ stride(1) = 1 → 跳过 1 个元素才能到下一列
