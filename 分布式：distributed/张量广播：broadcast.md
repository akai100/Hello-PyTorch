```torch.distributed.broadcast``` æ˜¯ PyTorch åˆ†å¸ƒå¼è®­ç»ƒä¸­æœ€åŸºç¡€ã€æœ€å¸¸ç”¨çš„é€šä¿¡åŸè¯­ä¹‹ä¸€ï¼Œä¸»è¦ç”¨äº æŠŠä¸€ä¸ªè¿›ç¨‹ï¼ˆrankï¼‰ä¸Šçš„å¼ é‡å¹¿æ’­åˆ°åŒä¸€è¿›ç¨‹ç»„é‡Œçš„æ‰€æœ‰å…¶ä»–è¿›ç¨‹ã€‚

```python3
def broadcast(
    tensor: torch.Tensor,
    src: Optional[int] = None,
    group: Optional[ProcessGroup] = None,
    async_op: bool = False,
    group_src: Optional[int] = None,
):
```

```mermaid
graph TD
    A("Rank 0")
    B("Rank 0")
    C("Rank 1")
    D("Rank 2")
    E("Rank 3")
    A --> B
    A --> C
    A --> D
    A --> E
```

## å‚æ•°

+ tensor

  è¦å¹¿æ’­çš„å¼ é‡ï¼ˆæ‰€æœ‰ rank éƒ½å¿…é¡»æä¾›åŒ shape / dtype çš„ tensorï¼‰

+ src

  æºè¿›ç¨‹çš„ rankï¼ˆæ•°æ®æ¥è‡ªè¿™ä¸ª rankï¼‰

+ group

  è¿›ç¨‹ç»„ï¼Œé»˜è®¤æ˜¯ WORLD

+ async_op

  æ˜¯å¦å¼‚æ­¥æ‰§è¡Œï¼Œè¿”å› Work å¯¹è±¡


## ä½¿ç”¨

### åŸºæœ¬ä½¿ç”¨ï¼šå¹¿æ’­ä¸€ä¸ª Tensor

```python3
tensor = torch.zeros(3).cuda()

if rank == 0:
    tensor = torch.tensor([1., 2., 3.]).cuda()

dist.broadcast(tensor, src=0)

print(f"rank {rank}: {tensor}")
```

### å¼‚æ­¥å¹¿æ’­

```python3
work = dist.broadcast(tensor, src=0, async_op=True)

#
do_something()

work_wait()
```

## å…¸å‹ä½¿ç”¨åœºæ™¯

### åŒæ­¥æ¨¡å‹åˆå§‹åŒ–å‚æ•°

```python3
for param in model.parameters():
    dist.broadcast(param.data, src=0)
```

ğŸ‘‰ ç¡®ä¿æ‰€æœ‰è¿›ç¨‹çš„æ¨¡å‹åˆå§‹æƒé‡ä¸€è‡´

### 2ï¸âƒ£ å¹¿æ’­éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰

```python3
seed = torch.tensor([1234], device="cuda")

if rank == 0:
    seed = torch.tensor([torch.randint(0, 10000, (1,))], device="cuda")

dist.broadcast(seed, src=0)

torch.manual_seed(seed.item())
```

### 3ï¸âƒ£ å¹¿æ’­é…ç½®ä¿¡æ¯ / è¶…å‚æ•°

```python3
lr = torch.tensor([0.0], device="cuda")

if rank == 0:
    lr = torch.tensor([0.001], device="cuda")

dist.broadcast(lr, src=0)

```

### 4ï¸âƒ£ Checkpoint æ¢å¤æ—¶åŒæ­¥çŠ¶æ€

```python3
if rank == 0:
    ckpt = torch.load("model.pt")
    state = ckpt["epoch"]
else:
    state = torch.zeros(1, dtype=torch.long)

dist.broadcast(state, src=0)

```
